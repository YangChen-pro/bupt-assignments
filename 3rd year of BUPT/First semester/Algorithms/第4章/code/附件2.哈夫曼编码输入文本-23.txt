Informally, an algorithm is any welldefined computational procedure that takes some value, or set of values, as input and produces some value, or set of values, as output. An algorithm is thus a sequence of computational steps that transform the input into the output.
We can also view an algorithm as a tool for solving a well-specified computational problem. The statement of the problem specifies in general terms the desired input/output relationship. The algorithm describes a specific computational procedure for achieving that input/output relationship.
An algorithm is said to be correct if, for every input instance, it halts with the correct output. We say that a correct algorithm solves the given computational problem. An incorrect algorithm might not halt at all on some input instances, or it might halt with an answer other than the desired one. Contrary to what one might expect, incorrect algorithms can sometimes be useful, if their error rate can be controlled. We shall see an example of this in Chapter when we study algorithms for finding large prime numbers. Ordinarily, however, we shall be concerned only with correct algorithms.
Algorithms for optimization problems typically go through a sequence of steps, with a set of choices at each step. For many optimization problems, using dynamic programming to determine the best choices is overkill; simpler, more efficient algorithms will do. A greedy algorithm always makes the choice that looks best at the moment. That is, it makes a locally optimal choice in the hope that this choice will lead to a globally optimal solution. This chapter explores optimization problems that are solvable by greedy algorithms. 
Greedy algorithms do not always yield optimal solutions, but for many problems they do. We shall first examine in Section 16.1 a simple but nontrivial problem, the activity-selection problem, for which a greedy algorithm efficiently computes a solution. We shall arrive at the greedy algorithm by first considering a dynamic-programming solution and then showing that we can always make greedy choices to arrive at an optimal solution. Section 16.2 reviews the basic elements of the greedy approach, giving a more direct approach to proving greedy algorithms correct than the dynamic-programming-based process of Section 16.1. Section 16.3 presents an important application of greedy techniques: the design of data-compression (Huffman) codes. In Section 16.4, we investigate some of the theory underlying combinatorial structures called "matroids" for which a greedy algorithm always produces an optimal solution. Finally, Section 16.5 illustrates the application of matroids using a problem of scheduling unit-time tasks with deadlines and penalties. The greedy method is quite powerful and works well for a wide range of problems. Later chapters will present many algorithms that can be viewed as applications of the greedy method, including minimum-spanning-tree algorithms , Dijkstra's algorithm for shortest paths from a single source, and Chvatal's greedy set-covering heuristic. Minimum-spanning-tree algorithms are a classic example of the greedy method. 
A greedy algorithm obtains an optimal solution to aproblem by making asequence of choices. For each decision point in thealgorithm, the choice that seems best at the moment is chosen. This heuristic strategy does not alwaysproduce an optimal solution,but as we saw in the activity-selection problem, sometimes it does. This section discusses some of the general properties of greedy methods. The process that we followed in Section 16.1 to develop agreedy algorithm was a bit more involved than is typical.
We went through the following steps:
(1) Determine the optimal substructure of the problem.
(2)Develop a recursive solution.
(3)Prove that at any stage of the recursion, one of the optimal choices is the greedy choice. Thus, it is always safe to make the greedy choice.
(4)Show that all but one of the subproblems induced by having made the greedy choice are empty.
(5)Develop a recursive algorithm that implements the greedy strategy.
(6)Convert the recursive algorithm to an iterative algorithm.
In going through these steps, we saw in great detail the dynamic-programming underpinnings of a greedy algorithm. In practice, however, we usually streamline the above steps when designing agreedy algorithm. We develop our substructure with an eye toward making a greedy choice that leaves just one subproblem to solve optimally. 
Greedy-choice property:
The first key ingredient is the greedy-choice property: aglobally optimal solution can be arrived at by making a locally optimal (greedy) choice. In other words, when we are considering which choice to make, we make the choice that looks best in the current problem, without considering results from subproblems. Here is where greedy algorithms differ from dynamic programming. In dynamic programming, we make achoice at each step, but the choice usually depends on the solutions to subproblems. Consequently, we typically solve dynamic-programming problems in a bottom-up manner,progressing from smaller subproblems to larger subproblems. In a greedy algorithm, we make whatever choice seems best at the moment and then solve the subproblem arising after thechoice ismade. The choice made by a greedy algorithm may depend on choices so far, but it cannot depend on any future choices or on the solutions to subproblems. Thus, unlike dynamic programming, which solves the subproblems bottom up, a greedy strategy usually 
proesses in a top-down fashion, making one greedy choice after another, reducing each given problem instance to a smaller one. 
Of course, we must prove that a greedy choice at each step yields a globally optimal solution, and this is where cleverness may be required. 
Optimal substructure: 
A problem exhibits optimal substructure if an optimal solution to the problem contains within it optimal solutions to subproblems. This property is a key ingredient of assessing the applicability of dynamic programming as well as greedy algorithms. 
We usually use a more direct approach regarding optimal substructure when applying it to greedy algorithms. As mentioned above, we have the luxury of assuming that we arrived at a subproblem by having made the greedy choice in the original problem. All we really need to do is argue that an optimal solution to the subproblem, combined with the greedy choice already made, yields an optimal solution to the original problem. This scheme implicitly uses induction on the subproblems to prove that making the greedy choice at every step produces an optimal solution.
Greedy versus dynamic programming:
Because the optimal-substructure property is exploited by both the greedy and dynamic-programming strategies, one might be tempted to generate a dynamic-programming solution to a problem when a greedy solution suffices, or one might mistakenly think that a greedy solution works when in fact a dynamic-programming solution is required. To illustrate the subtleties between the two techniques, let us investigate two variants of a classical optimization problem.



.